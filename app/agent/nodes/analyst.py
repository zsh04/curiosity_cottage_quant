import logging
import numpy as np
import time
from typing import Optional
import pandas as pd

from app.agent.state import AgentState, OrderSide, TradingStatus
from app.adapters.market import MarketAdapter
from app.adapters.llm import LLMAdapter
from app.adapters.sentiment import SentimentAdapter
from app.adapters.chronos import ChronosAdapter
from app.lib.kalman import KinematicKalmanFilter
from app.lib.memory import FractalMemory
from app.lib.physics.heavy_tail import HeavyTailEstimator
from app.services.global_state import get_global_state_service, get_current_snapshot_id
from app.strategies import STRATEGY_REGISTRY
from app.strategies.base import BaseStrategy

logger = logging.getLogger(__name__)


class AnalystAgent:
    """
    The Analyst Node acts as the Quantitative Strategist.
    It implements the 'Sensor Fusion' pipeline:
    Data -> Fractal Differentiation -> Kalman Filter -> Chronos Forecast -> LLM Inference -> Signal.
    """

    def __init__(self):
        self.market = MarketAdapter()
        self.llm = LLMAdapter()
        self.sentiment = SentimentAdapter()
        self.chronos = ChronosAdapter()
        self.kf = KinematicKalmanFilter()
        self.physics = HeavyTailEstimator(window_size=100)  # Window for Hill Estimator

    def run_tournament(self, prices_df: pd.DataFrame) -> tuple[BaseStrategy, float]:
        """
        Runs a micro-backtest on the last 200 bars for each strategy in the registry.
        Returns the winning strategy and its Sharpe Ratio.
        """
        best_strategy = None
        best_score = -float("inf")

        # Limit to last 200 bars
        history = prices_df.tail(200).copy()
        if len(history) < 50:
            # Not enough data for meaningful tournament, return first default
            return STRATEGY_REGISTRY[0], 0.0

        for strategy in STRATEGY_REGISTRY:
            with self.tracer.start_as_current_span(f"tournament.{strategy.name}"):
                strategy_returns = []

                # Micro-Backtest Loop
                # Optimization: We only re-calculate signal every bar.
                # This approximates the rolling performance.
                # For t in range(window, len):
                #   slice = history[:t]
                #   sig = strat.calc(slice)
                #   ret = sig * next_return

                # Note: This loop can be slow if STRATEGIES are slow.
                # Given strict verified strategies, they should be reasonably fast.

                # We start from index 20 (assuming min window) to end-1
                # We need next period return to calculate PnL.

                # Vectorized backtest is not possible without refactoring strategies to return series.
                # We proceed with loop.

                signals = []
                # Pre-calculate returns series for efficiency
                market_returns = history["returns"].values

                # Iterate through the history to generate signals
                # We simulate walking forward
                for t in range(20, len(history)):
                    # Slice history up to t (inclusive of t as "current" info)
                    # We are deciding for t+1
                    window_slice = history.iloc[: t + 1]

                    try:
                        sig = strategy.calculate_signal(window_slice)
                    except Exception:
                        sig = 0.0
                    signals.append(sig)

                # Trim signals to match returns alignment
                # Signal at t generates return at t+1
                # signals index 0 corresponds to history index 20 (decision made at t=20)
                # market_returns[21] is the return we caputre.

                # market_returns index alignment:
                # history indexes: 0..N-1
                # Loop t: 20..N-1
                # signals: [sig_20, sig_21, ... sig_N-1]
                # returns needed: [ret_21, ret_22, ... ret_N] (Wait, ret_N is out of bounds?)
                # history['returns'] at index i is return from i-1 to i.
                # If we make decision at t (using close_t), we capture return at t+1.
                # So we associate sig_t with ret_{t+1}.

                strat_pnl = []
                for i, sig in enumerate(signals):
                    # Original index of decision: t = 20 + i
                    # Return realized index: t + 1 = 21 + i
                    ret_idx = 20 + i + 1
                    if ret_idx < len(market_returns):
                        r = market_returns[ret_idx]
                        strat_pnl.append(sig * r)

                # Calculate Score (Sharpe)
                if not strat_pnl:
                    score = 0.0
                else:
                    avg_ret = np.mean(strat_pnl)
                    std_ret = np.std(strat_pnl)
                    if std_ret < 1e-9:
                        score = 0.0
                    else:
                        score = avg_ret / std_ret

                if score > best_score:
                    best_score = score
                    best_strategy = strategy

        if best_strategy is None:
            # Fallback
            return STRATEGY_REGISTRY[0], 0.0

        return best_strategy, best_score

    def analyze(self, state: AgentState) -> AgentState:
        symbol = state.get("symbol", "SPY")
        start_time = time.time()
        success = True
        error_msg = None

        try:
            # --- Step 1: Sensory (Data Collection) ---
            current_price = self.market.get_current_price(symbol)
            historic_returns = self.market.get_historic_returns(symbol)

            # CRITICAL: Update state immediately
            state["price"] = current_price
            state["historic_returns"] = historic_returns

            # Reconstruct raw price history from returns
            prices = [current_price]
            for r in reversed(historic_returns):
                denom = 1.0 + r
                if denom == 0:
                    prices.append(prices[-1])
                else:
                    prev_price = prices[-1] / denom
                    prices.append(prev_price)
            prices_chronological = list(reversed(prices))

            # Create DataFrame for Strategies
            # Strategies expect 'close' col.
            # We also need 'returns' for the tournament PnL.
            # historic_returns are [newest...oldest] usually in this codebase?
            # Let's double check `market.get_historic_returns`.
            # In `analyze` above: `for r in reversed(historic_returns)` -> suggests historic_returns is [newest, t-1, t-2...]
            # because `prices.append(prev_price)`.
            # So `historic_returns[0]` is return from t-1 to t.
            # `prices_chronological` is [oldest ... newest].
            # We need to align them.

            df_len = len(prices_chronological)
            timestamps = pd.date_range(
                end=pd.Timestamp.now(), periods=df_len, freq="min"
            )  # Mock index

            prices_df = pd.DataFrame({"close": prices_chronological}, index=timestamps)

            # Calculate returns column from chronological prices to be sure of alignment
            prices_df["returns"] = prices_df["close"].pct_change().fillna(0.0)

            # --- Step 2: STRATEGY TOURNAMENT ---
            # Replaces Manual Sensor Fusion for Decision Making
            winner, score = self.run_tournament(prices_df)

            logger.info(f"ðŸ† Tournament Winner: {winner.name} (Score: {score:.2f})")

            # Run Winner Logic on Full Data
            final_signal = winner.calculate_signal(prices_df)

            # Map Signal to OrderSide
            signal_side = OrderSide.FLAT.value
            if final_signal > 0.3:  # Thresholding
                signal_side = OrderSide.BUY.value
            elif final_signal < -0.3:
                signal_side = OrderSide.SELL.value

            confidence = abs(final_signal)

            # --- Update State ---
            state["active_strategy"] = winner.name
            state["strategy_score"] = score
            state["signal_side"] = signal_side
            state["signal_confidence"] = confidence
            state["reasoning"] = (
                f"Winner: {winner.name} (Sharpe {score:.2f}). Signal: {final_signal:.2f}"
            )

            # Optional: Keep "Sensory" context updates (Physics/Sentiment/Chronos)
            # if we want to retain observability, but for this refactor we focus on the Strategy.
            # I will Retain the Physics/Chaos updates for the UI but remove the "God Prompt" LLM call.

            # ... (Retaining essential context updates for UI if requested,
            # but user said 'Refactor Analyst Agent to run ... and Update analyze'.
            # I'll keep the Physics parts as they are useful for "Analysis" even if not used for signal.)

            # Minimal necessary updates to retain system integrity:
            # We can skip the expensive LLM God Prompt since we have a signal.

            # Logging
            log_msg = (
                f"ANALYST: ðŸ† {winner.name} (Sharpe {score:.2f}) | "
                f"Signal {signal_side} ({final_signal:.2f})"
            )
            print(log_msg)
            if "messages" not in state:
                state["messages"] = []
            state["messages"].append(log_msg)

        except Exception as e:
            success = False
            error_msg = f"ANALYST: ðŸ’¥ CRASH: {e}"
            print(error_msg)
            logger.exception(error_msg)
            state["signal_side"] = OrderSide.FLAT.value
            state["reasoning"] = f"Error: {e}"
            if "messages" not in state:
                state["messages"] = []
            state["messages"].append(error_msg)

        finally:
            latency = (time.time() - start_time) * 1000
            state_service = get_global_state_service()
            snapshot_id = get_current_snapshot_id()
            if state_service and snapshot_id:
                state_service.save_agent_metrics(
                    snapshot_id=snapshot_id,
                    agent_name="analyst",
                    latency_ms=latency,
                    success=success,
                    output_data={
                        "signal_side": state.get("signal_side"),
                        "confidence": state.get("signal_confidence"),
                        "active_strategy": state.get("active_strategy"),
                        "strategy_score": state.get("strategy_score"),
                    },
                    error=error_msg,
                )

        return state


def analyst_node(state: AgentState) -> AgentState:
    agent = AnalystAgent()
    return agent.analyze(state)
